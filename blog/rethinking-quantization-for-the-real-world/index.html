<!DOCTYPE html>
<!-- _layouts/distill.html --><html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script>
  let thunk = () => {
    let trimIt = (e) => e.trim();
    let getText = (e) => e.innerText;
    let splitNameAndFamilyName = (e) => {
      let splitted = e.split(" ");

      let fnames = splitted.slice(0, -1).join(" ");
      let lname = splitted.at(-1);

      return [lname, fnames];
    }

    let authors = Array.from(document.getElementsByClassName("author")).map(getText).map(trimIt).map(splitNameAndFamilyName);
    let firstAuthorLName = authors[0][0];
    let affiliationElements = Array.from(document.getElementsByClassName("affiliation")).filter(e => e.nodeName === "P").map(getText).map(trimIt);

    // getting stuff directly from Jekyll
    let publishedWhen = "May 30, 2025";
    let title = "Rethinking Quantization for the Real World";
    let description = "This blog post highlights the fact that most existing quantization techniques have been developed and validated under clean, balanced benchmark settings. It critically examines whether these methods can be reliably applied in real-world environments, where data imbalance and custom evaluation metrics constraints are the norm. Furthermore, it proposes ideas for partially mitigating the impact of real-world data imbalance during the Mixed Precision Quantization (MPQ) process.";

    {
      let authorsBibtex = authors.map(e => `${e[0]}, ${e[1]}`).join(" and ");
      let bibtexTitleShorthand = (firstAuthorLName +
              "2025" +
              title.split(" ").slice(0, 3).join("")
      ).replace(" ", "").replace(/[\p{P}$+<=>^`|~]/gu, '').toLowerCase().trim();
      let bibtexTemplate = `
@inproceedings{${bibtexTitleShorthand},
  author = {${authorsBibtex}},
  title = {${title}},
  abstract = {${description}},
  booktitle = {ICLR Blogposts 2025},
  year = {2025},
  date = {${publishedWhen}},
  note = {${window.location.href}},
  url  = {${window.location.href}}
}
  `.trim();
      document.getElementById("bibtex-box").innerText = bibtexTemplate;
    }

    {
      let academicLFI = authors.map(e => e[0]);
      {
        if (academicLFI.length > 2) academicLFI = academicLFI[0] + ", et al.";
        else if (academicLFI.length == 2) academicLFI = academicLFI[0] + " & " + academicLFI[1];
        else academicLFI = academicLFI[0];
      }
      let academicTemplate = `
${academicLFI}, "${title}", ICLR Blogposts, 2025.
`.trim();
      document.getElementById("bibtex-academic-attribution").innerText = academicTemplate;
    }
  };

  document.addEventListener('readystatechange', function(event) {
    if (document.readyState === "complete") {
      thunk();
    }
  });



</script>

      <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Rethinking Quantization for the Real World | ICLR Blogposts 2025</title>
    <meta name="author" content="ICLR  Blog">
    <meta name="description" content="This blog post highlights the fact that most existing quantization techniques have been developed and validated under clean, balanced benchmark settings. It critically examines whether these methods can be reliably applied in real-world environments, where data imbalance and custom evaluation metrics constraints are the norm. Furthermore, it proposes ideas for partially mitigating the impact of real-world data imbalance during the Mixed Precision Quantization (MPQ) process.">
    <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/iclr_favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://mjlee5982.github.io/blog/rethinking-quantization-for-the-real-world/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  

  <d-front-matter>
    <script async type="text/json">{
      "title": "Rethinking Quantization for the Real World",
      "description": "This blog post highlights the fact that most existing quantization techniques have been developed and validated under clean, balanced benchmark settings. It critically examines whether these methods can be reliably applied in real-world environments, where data imbalance and custom evaluation metrics constraints are the norm. Furthermore, it proposes ideas for partially mitigating the impact of real-world data imbalance during the Mixed Precision Quantization (MPQ) process.",
      "published": "May 30, 2025",
      "authors": [
        {
          "author": "Myeongjun Lee",
          "authorURL": "https://github.com/mjlee5982",
          "affiliations": [
            {
              "name": "POHANG POSTECH",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  </head>
<body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">ICLR Blogposts 2025</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <!-- [#&lt;Jekyll::Page @relative_path=&quot;404.html&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/about.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/call.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/dropdown.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;index.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;assets/css/main.scss&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/reviewer_guidelines.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;robots.txt&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/submitting.md&quot;&gt;, #&lt;Jekyll:Archive @type=year @title={:year=&gt;&quot;2024&quot;} @data={&quot;layout&quot;=&gt;&quot;archive-year&quot;}&gt;, #&lt;Jekyll:Archive @type=year @title={:year=&gt;&quot;2025&quot;} @data={&quot;layout&quot;=&gt;&quot;archive-year&quot;}&gt;, #&lt;JekyllRedirectFrom::PageWithoutAFile @relative_path=&quot;redirects.json&quot;&gt;, #&lt;JekyllFeed::PageWithoutAFile @relative_path=&quot;feed.xml&quot;&gt;, #&lt;Jekyll::PaginateV2::Generator::PaginationPage @relative_path=&quot;.html&quot;&gt;, #&lt;Jekyll::PageWithoutAFile @relative_path=&quot;sitemap.xml&quot;&gt;] -->

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/2023/about">about</a>
              </li> -->
              <!-- Call -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/2023/call">call</a>
              </li> -->
              <!-- submissions -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/2023/submissions/">submissions</a>
              </li> -->
              <!-- Blog -->
              <!-- <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li> -->
              <!-- 2022 -->
              <!-- <li class="nav-item">
                <a class="nav-link" href="https://iclr-blog-track.github.io/home/">2022 edition <u>⤤</u><span class="sr-only">(current)</span></a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/about/">about</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/call/">call for blogposts</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/submitting/">submitting</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/reviewing/">reviewing</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/index.html">blog</a>
              </li>
                <li class="nav-item dropdown ">
                  <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a>
                  <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                    <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a>
                  </div>
                </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Rethinking Quantization for the Real World</h1>
        <p>This blog post highlights the fact that most existing quantization techniques have been developed and validated under clean, balanced benchmark settings. It critically examines whether these methods can be reliably applied in real-world environments, where data imbalance and custom evaluation metrics constraints are the norm. Furthermore, it proposes ideas for partially mitigating the impact of real-world data imbalance during the Mixed Precision Quantization (MPQ) process.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#introduction">Introduction</a></div>
            <div><a href="#background">Background</a></div>
            <ul>
              <li><a href="#application-driven-machine-learning-adml">Application Driven Machine Learning (ADML)</a></li>
              <li><a href="#long-tail-distribution-ltd">Long Tail Distribution (LTD)</a></li>
              <li><a href="#decoupling-representation-and-classifier-for-lonng-tailed-recognition">Decoupling Representation and Classifier for Lonng-Tailed Recognition</a></li>
              <li><a href="#quantization-and-mixed-precision-quantization-mpq">Quantization and Mixed Precision Quantization (MPQ)</a></li>
              
            </ul>
<div><a href="#insight-from-experiments">Insight from Experiments</a></div>
            <ul>
              <li><a href="#are-traditional-evaluation-methods-applicable">Are Traditional Evaluation Methods Applicable?</a></li>
              <li><a href="#can-effecitve-training-methods-address-ltd-issues">Can Effecitve Training Methods Address LTD Issues?</a></li>
              <li><a href="#does-ltd-impact-quantized-modes">Does LTD Impact Quantized Modes?</a></li>
              <li><a href="#can-quantization-process-mitigate-real-world-ltd-issues">Can Quantization Process Mitigate REal-World LTD Issues?</a></li>
              
            </ul>
<div><a href="#conclusion">Conclusion</a></div>
            <ul>
              <li><a href="#what-problem-is-this-work-trying-to-tackle">What problem is this work trying to tackle?</a></li>
              <li><a href="#that-contributions-did-this-work-make-and-what-impact-should-this-work-have">That contributions did this work make, and what impact should this work have?</a></li>
              <li><a href="#how-new-is-this-effort">How new is this effort?</a></li>
              <li><a href="#what-are-the-limitations-of-this-work">what are the limitations of this work?</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="1-introduction">1. Introduction</h2>
<p>This blog post offers a fresh perspective on traditional machine learning concepts and techniques by questioning whether quantization can be effectively applied to models trained on real-world, application-driven data, from the viewpoint of Application-Driven Machine Learning (ADML) <d-cite key="rolnick2024application"></d-cite>. To date, research on quantization techniques for CNN models has largely focused on a limited set of widely used benchmark datasets, such as CIFAR-10 and ImageNet, and have been evaluated using narrow metrics such as accuracy and compression rate. However, real world applications often involve datasets with very different characteristics and evaluation criteria. For example, many naturally occurring datasets exhibit long-tailed distributions, where a few classes dominate the data and the majority of classes appear infrequently. These imbalanced distributions often cause models to focus on high-frequency classes during training, leading to severely degraded performance on rare classes. When quantization is applied to models trained on such long-tailed data, it’s possible that the overall accuracy may appear stable, yet performance on tail classes may deteriorate significantly. This highlights a critical limitation in how quantized models are evaluated and interpreted, especially under real-world constraints.</p>

<h2 id="2-background">2. Background</h2>

<h3 id="21-application-driven-machine-learning-adml">2.1 Application Driven Machine Learning (ADML)</h3>
<p>Application-Driven Machine Learning (ADML) is an approach to machine learning research emphasizing the design of algorithms specifically tailored to address real-world problems, rather than optimizing performance solely on standardized benchmark datasets. Unlike traditional methods-driven research, which prioritizes generalized metrics like accuracy and loss on clean, well-structured datasets (e.g., CIFAR-10, ImageNet), ADML explicitly incorporates domain-specific considerations, such as custom evaluation criteria, data characteristics, and user-defined constraints <d-cite key="rolnick2024application"></d-cite>. This paradigm is crucial because it acknowledges that real-world tasks frequently differ substantially from benchmark scenarios. For instance, tasks in remote sensing, healthcare, or biodiversity monitoring often require models to consider computational constraints, domain knowledge, and specialized metrics like uncertainty quantification or cost-sensitive accuracy. Recognizing and addressing these practical considerations significantly enhances the applicability and effectiveness of machine learning solutions in real-world environments.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig1.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 1: Methods-Driven ML vs Application-Driven ML 
</div>

<h3 id="22-long-tail-distribution-ltd">2.2 Long Tail Distribution (LTD)</h3>
<p>Long-tail distribution (LTD) describes a scenario where a small number of classes dominate the dataset (head classes), while the majority of classes have significantly fewer samples (tail classes) <d-cite key="kang2020decoupling"></d-cite>. Such distributions are ubiquitous in real-world datasets like iNaturalist (biodiversity data), ImageNet-LT, and Places-LT, where many categories are represented by very few examples. LTD poses substantial challenges for machine learning models, which tend to overfit on head classes while underperforming on tail classes due to insufficient exposure during training. Consequently, the overall accuracy metric often masks severe performance degradation on rare classes, making standard evaluation metrics unreliable indicators of true model effectiveness. Addressing LTD requires strategies such as data re-sampling, class-balanced losses, and specialized evaluation protocols that better reflect performance across all classes, ensuring equitable attention to rare but potentially critical categories.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig2.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 2: Long Tail Distribution (LTD) 
</div>

<h3 id="23-decoupling-representation-and-classifier-for-long-tailed-recognition">2.3 Decoupling Representation and Classifier for Long-Tailed Recognition</h3>
<p>The concept of “Decoupling Representation and Classifier” for long-tailed recognition involves separating the training procedure into two distinct phases: representation learning and classifier training <d-cite key="kang2020decoupling"></d-cite>. Traditionally, models learn representations and classifiers jointly, which can obscure the sources of performance improvements. The decoupling approach demonstrates that effective representation learning does not necessarily require complex class-balancing strategies. Instead, training representations with simple instance-balanced sampling can yield robust features applicable across both frequent and infrequent classes. Subsequently, a classifier is trained or adjusted separately using methods like class-balanced re-training, nearest class mean classification, or weight normalization. Experimental studies using this decoupling strategy have shown substantial improvements over traditional methods on benchmarks such as ImageNet-LT, iNaturalist, and Places-LT, clearly illustrating its potential to effectively address long-tailed challenges without resorting to overly complex methods.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig3.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 3: Decoupling representation and classifier for long-tailed recognition.
    In Stage 1, feature extraction and the downstream task are simultaneously trained using an imbalanced dataset. In Stage 2, only the downstream task is separately trained.
</div>

<h3 id="24-quantization--mixed-precision-quantization-mpq">2.4. Quantization &amp; Mixed Precision Quantization (MPQ)</h3>
<p>Quantization refers to reducing the numerical precision (bit-width) used to represent neural network weights and activations, aiming to decrease computational and memory demands, which are critical for deploying models in resource-constrained environments. Mixed Precision Quantization (MPQ) takes this concept further by assigning different bit-widths to various layers or components within a neural network, optimizing the trade-off between model accuracy and compression efficiency <d-cite key="wang2019haq"></d-cite> <d-cite key="tang2023mixed"></d-cite>. This approach leverages the observation that different layers have varying sensitivities to quantization for example, some layers can be heavily quantized without significant performance loss, whereas others require higher precision to maintain accuracy. By carefully choosing bit-width assignments based on layer importance or sensitivity, MPQ can achieve substantial reductions in memory and energy usage while maintaining acceptable accuracy, making it a highly effective method for efficient, real-world deployment of machine learning models.</p>

<h2 id="3-insight-from-experiments">3. Insight from Experiments</h2>
<p>This section provides insights derived from a series of straightforward and clear experiments aimed at understanding how quantization, a key technique in efficient machine learning, behaves under real-world data imbalance conditions like long-tail distributions (LTD). We investigated characteristic challenges and potential solutions when traditional evaluation environments and metrics are applied to LTD.
The experimental setup was designed for ease of reproducibility and to provide a simple, intuitive sensitivity evaluation approach for setting layer-wise bit-widths in Mixed Precision Quantization (MPQ). We created a CIFAR10-LT dataset and used the VGG16 CNN model for experiments.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig4-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig4-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig4-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig4.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 4: CIFAR10-LT(Long Tailed Distribution). 
    The CIFAR10 dataset is a balanced dataset consisting of 10 classes with 5000 images per class. CIFAR10-LT is generated by intentionally creating an imbalance in the number of images per class to form a long-tail distribution. For instance, with an imbalance factor (imb_factor) of 0.01, the ratio between Class 0 and Class 9 is 100:1
</div>

<h3 id="31-are-traditional-evaluation-methods-applicable">3.1 Are Traditional Evaluation Methods Applicable?</h3>
<p>Given that traditional quantization techniques have been developed and validated on balanced and well-structured datasets, we questioned whether existing evaluation metrics remain effective when applied to models trained on LTD datasets.
Normalized overall accuracy, commonly used as a performance metric for CNN-based image classification tasks, proved inadequate for evaluating models trained on LTD datasets. We generated CIFAR10-LT datasets with imbalance factors of 0.1(the number of image gap between head and tail is 10 times), 0.02, and 0.01. In addition to the overall accuracy (Acc(Norm)), we separately measured accuracy for head classes (approximately 80% of data) and tail classes (remaining 20%). The results revealed that Acc(Norm) failed to reflect performance accurately for head (Acc(Head)) and tail (Acc(Tail)) classes, particularly as imbalance factors increased. This demonstrates the necessity of evaluating head and tail classes separately in LTD scenarios.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig5-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig5-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig5.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 5: Comparison of Normal Accuracy with Head &amp; Tail Class Accuracy.
    As the imbalance factor increases, the performance gap between the head and tail classes becomes more significant. Normal Accuracy fails to accurately reflect this imbalanced performance.
</div>

<h3 id="32-can-effective-training-methods-address-ltd-issues">3.2 Can Effective Training Methods Address LTD Issues?</h3>
<p>Several methodologies have been proposed to tackle LTD datasets commonly found in real-world scenarios. Prominent methods include data-level solutions like re-sampling to balance head and tail classes, cost-sensitive learning to adjust class-specific loss during training, and transfer learning methods to leverage information from head classes to enhance tail class performance.
The Decoupling Representation and Classifier method, presented at ICLR 2020, demonstrated substantial performance improvements on LTD datasets by training representations (feature extractors) using instance-balanced sampling and separately retraining classifiers using class-balanced sampling. Using this method, we confirmed substantial performance improvements on CIFAR10-LT with imbalance factors of 0.1, 0.02, and 0.01. The improvement was more significant with higher imbalance factors, effectively narrowing the performance gap between head and tail classes compared to baseline methods. However, despite notable improvements, data imbalance persisted, especially evident at an imbalance factor of 0.01, where the accuracy gap between head and tail exceeded 10%. This indicates that while techniques like Decoupling Learning substantially mitigate data imbalance, significant imbalance still poses challenges.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table1.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Table 1: Performance Changes Due to Decoupling Learning
</div>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig6-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig6-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig6-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig6.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 6: Accuracy Changes of Head and Tail Classes due to Decoupling Learning.
    Decoupling learning improves performance imbalance caused by LTD. However, a residual imbalance in model performance persists when the imbalance factor is above 0.01.
</div>

<h3 id="33-does-ltd-impact-quantized-models">3.3 Does LTD Impact Quantized Models?</h3>
<p>Previous experiments revealed that CNN models trained on LTD datasets exhibited a bias toward head classes, leading to poorer tail class performance. While training methods such as Decoupling Learning improved performance, severe data imbalance inevitably resulted in notable performance disparities. This raised questions about how such disparities manifest in quantized models. We hypothesized and experimentally confirmed that tail class performance degradation becomes more pronounced relative to head class performance as bit precision decreases (from 32-bit to 8-bit, 6-bit, and 4-bit quantization). Additionally, models improved through Decoupling Learning showed smaller performance reductions in tail classes compared to baseline models at lower bit precisions. These findings clearly illustrate that quantization exacerbates data imbalance issues, particularly impacting tail class performance.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig7-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig7-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig7-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 7: Performance Degradation of Head and Tail Classes in a Regular Model due to Quantization.
    When quantizing from 32-bit to 6-bit, head class performance decreases by 0.5%, while tail class performance decreases by 6.5%.
</div>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig8-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig8-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig8-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig8.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 8: Performance Degradation of Head and Tail Classes in a Decoupling Learning Model due to Quantization.
    Quantizing from 32-bit to 6-bit causes a performance reduction of 2.7% in head classes and 6.6% in tail classes, further widening the performance gap between head and tail classes.
</div>

<h3 id="34-can-quantization-processes-mitigate-real-world-ltd-issues">3.4 Can Quantization Processes Mitigate Real-World LTD Issues?</h3>
<p>Considering the prevalence of LTD datasets across various real-world applications, and acknowledging that sampling and training techniques such as Decoupling Learning cannot entirely resolve performance disparities between head and tail classes, we explored whether the quantization process itself could further mitigate these disparities. Is there a simpler and more efficient way to reduce tail class performance degradation during quantization without extensive retraining?
Our experimental results offer a clear answer: Mixed Precision Quantization (MPQ) can indeed partially address tail class performance issues during quantization. We propose repurposing the existing Mixed Precision Quantization (MPQ) technique, originally designed to improve computational cost and energy efficiency, as a novel approach to address data imbalance issues that arise during the LTD based model compression process. In case of MPQ, to assign appropriate bit-widths to each layer, sensitivity analysis is required <d-cite key="yeji2025amc"></d-cite>. In our approach, we incorporate tail-class performance indicators into the analysis, guiding the model to reduce its overemphasis on head classes and instead improve the performance on tail classes, effectively driving compression in a more balanced direction. This approach significantly improved tail class performance without additional training overhead associated with post-training quantization (PTQ). Compared to uniform quantization, this imbalance-aware MPQ showed negligible overall accuracy degradation while effectively reducing the performance gap between head and tail classes during the quantization process.</p>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/table2.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Table 2: MPQ Model Performance Based on Sensitivity Evaluation Methods.
    The performance difference between MPQ evaluated with Normal accuracy and MPQ evaluated with Tail class accuracy is negligible compared to the decoupling learning model (baseline).
</div>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig9_new-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig9_new-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig9_new-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig9_new.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 9: Rank Changes Based on Tail Performance (Top 9).
    Rank changes between sensitivity evaluations based on Normal Accuracy and Tail class Accuracy were observed, suggesting that MPQ can be effectively tailored to enhance tail class performance.
</div>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig10_new-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig10_new-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig10_new-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig10_new.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 10: Class-wise Performance of MPQ Applied Model for Tail Class Improvement.
    Performance reduction in head classes (Class 0~3) and performance improvement in tail classes (Class 4~9) confirm that MPQ effectively mitigates performance disparities caused by data imbalance.
</div>

<div class="row mt-3">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig11-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig11-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig11-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2025-05-30-rethinking-quantization-for-the-real-world/fig11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
<div class="caption">
    Figure 11: Layer-wise Sensitivity Analysis Considering Data Imbalance.
</div>

<h2 id="4conclusion">4.Conclusion</h2>
<h3 id="41-what-problem-is-this-work-trying-to-tackle">4.1 What problem is this work trying to tackle?</h3>
<p>This research seeks solutions from the perspective of Application-Driven Machine Learning to address Long Tail Distribution (LTD), a common real-world problem, specifically through the application of model quantization. We experimentally explored the effects of data imbalance in LTD datasets on the conventional Decoupling Learning method and model quantization. Furthermore, we propose a method to resolve data imbalance using Mixed Precision Quantization (MPQ) at the quantization stage, rather than relying solely on data sampling or model training.</p>

<h3 id="42-that-contributions-did-this-work-make-and-what-impact-should-this-work-have">4.2 That contributions did this work make, and what impact should this work have?</h3>
<p>By exploring and proposing methods to overcome LTD problems at the quantization stage rather than during data sampling or model training, this work is expected to inspire future research aimed at addressing data imbalance issues during the model compression phase. Using CIFAR10-LT datasets and the VGG16 model, we conducted experiments that are relatively straightforward to reproduce and provided several significant results:</p>
<ul>
  <li>We confirmed that traditional performance metrics, such as model accuracy, are insufficient for accurately reflecting performance and data imbalance in LTD datasets. It is essential to separately measure and evaluate accuracy for head and tail classes to assess genuine performance improvements.</li>
  <li>Although significant performance improvements were achieved using the representative LTD dataset training method, Decoupling Learning, experiments demonstrated persistent data imbalance impacts when class imbalance exceeded 100 times.</li>
  <li>The impact of LTD datasets is also evident in quantized models, with tail class performance degradation becoming significantly more pronounced relative to head classes as bit precision decreases.</li>
  <li>Finally, to address LTD issues at the quantization stage, we propose an MPQ method. By incorporating tail class performance metrics into the MPQ sensitivity evaluation for layer-wise bit-width settings, we experimentally demonstrated the feasibility of an LTD-aware MPQ method.</li>
</ul>

<h3 id="43-how-new-is-this-effort">4.3 How new is this effort?</h3>
<p>While existing research primarily focuses on data sampling and training methods to solve LTD dataset problems, the approach proposed in this blog is novel in addressing data imbalance at the model compression stage through MPQ application. Additionally, by experimentally demonstrating the impact of data imbalance on quantized models, this research underscores the need for considering real-world problems like LTD at the quantization stage, suggesting new avenues for future research.</p>

<h3 id="44-what-are-the-limitations-of-this-work">4.4 what are the limitations of this work?</h3>
<p>Despite showing meaningful experimental results, the study is currently limited to the CIFAR10-LT dataset and a single VGG16 model. To generalize these findings more broadly, it will be necessary to expand experiments to include additional datasets and models.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <d-bibliography src="/assets/bibliography/2025-05-30-rethinking-quantization-for-the-real-world.bib"></d-bibliography>

    <d-article id="bibtex-container" class="related highlight">
      For attribution in academic contexts, please cite this work as
      <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre>

      BibTeX citation
      <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre>
    </d-article>


    <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async>
</script>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  </body>


</html>
