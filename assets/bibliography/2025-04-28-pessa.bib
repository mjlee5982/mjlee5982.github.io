@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}
@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{
anonymous2025hira,
title={Hi{RA}: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models},
author={Anonymous},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=TwJrTz9cRS}
}
@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}
@article{cherti2022reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  journal={arXiv preprint arXiv:2212.07143},
  year={2022}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}
@article{guo2023compresso,
  title={Compresso: Structured pruning with collaborative prompting learns compact large language models},
  author={Guo, Song and Xu, Jiahang and Zhang, Li Lyna and Yang, Mao},
  journal={arXiv preprint arXiv:2310.05015},
  year={2023}
}
@article{sun2024you,
  title={You only cache once: Decoder-decoder architectures for language models},
  author={Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2405.05254},
  year={2024}
}
@article{brandon2024reducing,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan},
  journal={arXiv preprint arXiv:2405.12981},
  year={2024}
}
@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}
@article{zuhri2024mlkv,
  title={MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding},
  author={Zuhri, Zayd Muhammad Kawakibi and Adilazuarda, Muhammad Farid and Purwarianti, Ayu and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2406.09297},
  year={2024}
}
@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}
@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}
@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@article{de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  year={2024}
}
@article{fu2024lazyllm,
  title={Lazyllm: Dynamic token pruning for efficient long context llm inference},
  author={Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
  journal={arXiv preprint arXiv:2407.14057},
  year={2024}
}
@article{jo2024a2sf,
  title={A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder},
  author={Jo, Hyun-rae and Shin, Dongkun},
  journal={arXiv preprint arXiv:2407.20485},
  year={2024}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}
@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}
@article{yu2024effectively,
  title={Effectively Compress KV Heads for LLM},
  author={Yu, Hao and Yang, Zelan and Li, Shen and Li, Yong and Wu, Jianxin},
  journal={arXiv preprint arXiv:2406.07056},
  year={2024}
}
@article{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}
@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}
@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}
@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}
@article{lawton2023neural,
  title={Neural architecture search for parameter-efficient fine-tuning of large pre-trained language models},
  author={Lawton, Neal and Kumar, Anoop and Thattai, Govind and Galstyan, Aram and Steeg, Greg Ver},
  journal={arXiv preprint arXiv:2305.16597},
  year={2023}
}
@article{zhao2020masking,
  title={Masking as an efficient alternative to finetuning for pretrained language models},
  author={Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2004.12406},
  year={2020}
}
@article{sung2021training,
  title={Training neural networks with fixed sparse masks},
  author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24193--24205},
  year={2021}
}
@article{ansell2021composable,
  title={Composable sparse fine-tuning for cross-lingual transfer},
  author={Ansell, Alan and Ponti, Edoardo Maria and Korhonen, Anna and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2110.07560},
  year={2021}
}
@article{xu2021raise,
  title={Raise a child in large language model: Towards effective and generalizable fine-tuning},
  author={Xu, Runxin and Luo, Fuli and Zhang, Zhiyuan and Tan, Chuanqi and Chang, Baobao and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2109.05687},
  year={2021}
}
@article{guo2020parameter,
  title={Parameter-efficient transfer learning with diff pruning},
  author={Guo, Demi and Rush, Alexander M and Kim, Yoon},
  journal={arXiv preprint arXiv:2012.07463},
  year={2020}
}
@inproceedings{fu2023effectiveness,
  title={On the effectiveness of parameter-efficient fine-tuning},
  author={Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={12799--12807},
  year={2023}
}

@article{hambardzumyan2021warp,
  title={Warp: Word-level adversarial reprogramming},
  author={Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  journal={arXiv preprint arXiv:2101.00121},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@article{vu2021spot,
  title={Spot: Better frozen model adaptation through soft prompt transfer},
  author={Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  journal={arXiv preprint arXiv:2110.07904},
  year={2021}
}
@article{asai2022attempt,
  title={ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts},
  author={Asai, Akari and Salehi, Mohammadreza and Peters, Matthew E and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2205.11961},
  year={2022}
}
@article{wang2023multitask,
  title={Multitask prompt tuning enables parameter-efficient transfer learning},
  author={Wang, Zhen and Panda, Rameswar and Karlinsky, Leonid and Feris, Rogerio and Sun, Huan and Kim, Yoon},
  journal={arXiv preprint arXiv:2303.02861},
  year={2023}
}




@article{qiu2023controlling,
  title={Controlling text-to-image diffusion by orthogonal finetuning},
  author={Qiu, Zeju and Liu, Weiyang and Feng, Haiwen and Xue, Yuxuan and Feng, Yao and Liu, Zhen and Zhang, Dan and Weller, Adrian and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={79320--79362},
  year={2023}
}
@article{liu2023parameter,
  title={Parameter-efficient orthogonal finetuning via butterfly factorization},
  author={Liu, Weiyang and Qiu, Zeju and Feng, Yao and Xiu, Yuliang and Xue, Yuxuan and Yu, Longhui and Feng, Haiwen and Liu, Zhen and Heo, Juyeon and Peng, Songyou and others},
  journal={arXiv preprint arXiv:2311.06243},
  year={2023}
}
@article{yuan2024bridging,
  title={Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation},
  author={Yuan, Shen and Liu, Haotian and Xu, Hongteng},
  journal={arXiv preprint arXiv:2405.17484},
  year={2024}
}
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}
@inproceedings{smith2023coda,
  title={Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning},
  author={Smith, James Seale and Karlinsky, Leonid and Gutta, Vyshnavi and Cascante-Bonilla, Paola and Kim, Donghyun and Arbelle, Assaf and Panda, Rameswar and Feris, Rogerio and Kira, Zsolt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11909--11919},
  year={2023}
}
@article{guo2024q,
  title={Q-Tuning: Queue-based prompt tuning for lifelong few-shot language learning},
  author={Guo, Yanhui and Xu, Shaoyuan and Fu, Jinmiao and Liu, Jia Kevin and Dong, Chaosheng and Wang, Bryan},
  year={2024}
}
@inproceedings{lee2022plug,
  title={Plug-and-Play Adaptation for Continuously-updated QA},
  author={Lee, Kyungjae and Han, Wookje and Hwang, Seung-won and Lee, Hwaran and Park, Joonsuk and Lee, Sang-Woo},
  booktitle={Findings of ACL 2022},
  pages={438--447},
  year={2022}
}
@article{zhao2023inrank,
  title={Inrank: Incremental low-rank learning},
  author={Zhao, Jiawei and Zhang, Yifei and Chen, Beidi and Sch{\"a}fer, Florian and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2306.11250},
  year={2023}
}
@inproceedings{wistuba2023continual,
  title={Continual Learning with Low Rank Adaptation},
  author={Wistuba, Martin and Balles, Lukas and Zappella, Giovanni and others},
  booktitle={NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models},
  year={2023}
}
@article{liang2024inflora,
  title={InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning},
  author={Liang, Yan-Shuo and Li, Wu-Jun},
  journal={arXiv preprint arXiv:2404.00228},
  year={2024}
}
@article{moral,
  title={MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning},
  author={Yang, Shu and Ali, Muhammad Asif and Wang, Cheng-Long and Hu, Lijie and Wang, Di},
  journal={arXiv preprint arXiv:2402.11260},
  year={2024}
}
@article{chen2024bayesian,
  title={Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting},
  author={Chen, Haolin and Garner, Philip N},
  journal={arXiv preprint arXiv:2402.12220},
  year={2024}
}
@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
}
@inproceedings{aljundi2018memory,
  title={Memory aware synapses: Learning what (not) to forget},
  author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle={Proceedings of the European conference on computer vision},
  pages={139--154},
  year={2018}
}
@inproceedings{chen2020recall,
  title={Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting},
  author={Chen, Sanyuan and Hou, Yutai and Cui, Yiming and Che, Wanxiang and Liu, Ting and Yu, Xiangzhan},
  booktitle={Proceedings of Conference on Empirical Methods in Natural Language Processing},
  pages={7870--7881},
  year={2020}
}
@inproceedings{huang2021continual,
  title={Continual Learning for Text Classification with Information Disentanglement Based Regularization},
  author={Huang, Yufan and Zhang, Yanzhe and Chen, Jiaao and Wang, Xuezhi and Yang, Diyi},
  booktitle={Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2736--2746},
  year={2021}
}
@inproceedings{zhang2023continual,
  title={Continual Named Entity Recognition without Catastrophic Forgetting},
  author={Zhang, Duzhen and Cong, Wei and Dong, Jiahua and Yu, Yahan and Chen, Xiuyi and Zhang, Yonggang and Fang, Zhen},
  booktitle={Proceedings of Conference on Empirical Methods in Natural Language Processing},
  pages={8186--8197},
  year={2023}
}
@article{shi2024unified,
  title={A unified approach to domain incremental learning with memory: Theory and algorithm},
  author={Shi, Haizhou and Wang, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{domain,
  title={Domain-lifelong learning for dialogue state tracking via knowledge preservation networks},
  author={Liu, Qingbin and Cao, Pengfei and Liu, Cao and Chen, Jiansong and Cai, Xunliang and Yang, Fan and He, Shizhu and Liu, Kang and Zhao, Jun},
  booktitle={Proceedings of Conference on Empirical Methods in Natural Language Processing},
  pages={2301--2311},
  year={2021}
}
@inproceedings{elle,
  title={ELLE: Efficient Lifelong Pre-training for Emerging Data},
  author={Qin, Yujia and Zhang, Jiajie and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={2789--2810},
  year={2022}
}
@inproceedings{Lifelongpretraining,
  title={Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora},
  author={Jin, Xisen and Zhang, Dejiao and Zhu, Henghui and Xiao, Wei and Li, Shang-Wen and Wei, Xiaokai and Arnold, Andrew and Ren, Xiang},
  booktitle={Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4764--4780},
  year={2022}
}
@article{corpusbrain,
  title={CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks},
  author={Guo, Jiafeng and Zhou, Changjiang and Zhang, Ruqing and Chen, Jiangui and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2402.16767},
  year={2024}
}
@inproceedings{sun2019lamol,
  title={LAMOL: LAnguage MOdeling for Lifelong Language Learning},
  author={Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{qin2021lfpt5,
  title={LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5},
  author={Qin, Chengwei and Joty, Shafiq},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{zhao2022prompt,
  title={Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue},
  author={Zhao, Yingxiu and Zheng, Yinhe and Tian, Zhiliang and Gao, Chang and Sun, Jian and Zhang, Nevin L},
  booktitle={Proceedings of Conference on Empirical Methods in Natural Language Processing},
  pages={11153--11169},
  year={2022}
}
@article{huang2024mitigating,
  title={Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal},
  author={Huang, Jianheng and Cui, Leyang and Wang, Ante and Yang, Chengyi and Liao, Xinting and Song, Linfeng and Yao, Junfeng and Su, Jinsong},
  journal={arXiv preprint arXiv:2403.01244},
  year={2024}
}
@article{shi2024continual,
  title={Continual learning of large language models: A comprehensive survey},
  author={Shi, Haizhou and Xu, Zihao and Wang, Hengyi and Qin, Weiyi and Wang, Wenyuan and Wang, Yibin and Wang, Zifeng and Ebrahimi, Sayna and Wang, Hao},
  journal={arXiv preprint arXiv:2404.16789},
  year={2024}
}
@inproceedings{
zhang2024cppo,
title={{CPPO}: Continual Learning for Reinforcement Learning with Human Feedback},
author={Han Zhang and Yu Lei and Lin Gui and Min Yang and Yulan He and Hui Wang and Ruifeng Xu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=86zAUE80pP}
}
@article{dsvi,
  author       = {DeepSeek-AI},
  title        = {DeepSeek {LLM:} Scaling Open-Source Language Models with Longtermism},
  journal      = {CoRR},
  volume       = {abs/2401.02954},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.02954},
}
@article{dsvii,
  author       = {DeepSeek-AI},
  title        = {DeepSeek-V2: {A} Strong, Economical, and Efficient Mixture-of-Experts
                  Language Model},
  journal      = {CoRR},
  volume       = {abs/2405.04434},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.04434},
}
@article{dsviii,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}
@article{gpt4,
  title={{GPT4} technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{gpt4o,
  title = {Hello {GPT-4o}},
  author = {OpenAI},
  url = {https://openai.com/index/hello-gpt-4o/},
  year={2024}
}
@misc{claude,
  title = {Introducing {Claude}},
  author = {Anthropic},
  institution = {Anthropic},
  url = {https://www.anthropic.com/index/introducing-claude},
  year={2023}
}
@misc{claude35sonnet,
  title = {Claude 3.5 Sonnet},
  author = {Anthropic},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  year={2024}
}
@misc{gemini,
  title = {Introducing Gemini: our largest and most capable AI model},
  author = {Google},
  url = {https://blog.google/technology/ai/google-gemini-ai/},
  year={2023}
}
@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{llama,
  title={{LLaMA}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{llama2,
  author       = {AI@Meta},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288}
}
@misc{llama3,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
@article{qwen,
  title={Qwen Technical Report},
  author={Qwen},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@misc{qwen1_5,
  title = {Introducing {Qwen1.5}},
  author = {Qwen},
  url = {https://qwenlm.github.io/blog/qwen1.5},
  year={2024}
}
@article{qwen2,
  author       = {Qwen},
  title        = {Qwen2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2407.10671},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.10671},
}
@misc{qwen2_5,
  title={Qwen2.5: A Party of Foundation Models},
  author={Qwen},
  year={2024},
  url = {https://qwenlm.github.io/blog/qwen2.5}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@misc{mixtral8x22b,
  title = {Cheaper, Better, Faster, Stronger: Continuing to push the frontier of AI and making it accessible to all},
  author = {Mistral},
  url = {https://mistral.ai/news/mixtral-8x22b},
  year={2024}
}
@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}
@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}
@article{wang2023trace,
  title={TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models},
  author={Wang, Xiao and Zhang, Yuansen and Chen, Tianze and others},
  journal={CoRR},
  year={2023}
}
@article{caccia2020online,
  title={Online fast adaptation and knowledge accumulation (osaka): a new approach to continual learning},
  author={Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and others},
  journal={NeurIPS},
  year={2020}
}
@inproceedings{mallya2018piggyback,
  title={Piggyback: Adapting a single network to multiple tasks by learning to mask weights},
  author={Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  booktitle={ECCV},
  year={2018}
}
@article{chaudhry2019tiny,
  title={On tiny episodic memories in continual learning},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and others},
  journal={arXiv:1902.10486},
  year={2019}
}
@article{kotha2023understanding,
  title={Understanding catastrophic forgetting in language models via implicit inference},
  author={Kotha, Suhas and Springer, Jacob Mitchell and Raghunathan, Aditi},
  journal={arXiv:2309.10105},
  year={2023}
}
@article{azerbayev2023llemma,
  title={Llemma: An open language model for mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and others},
  journal={CoRR},
  year={2023}
}
@article{kong2023tptu,
  title={TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems},
  author={Kong, Yilun and Ruan, Jingqing and Chen, Yihong and others},
  journal={arXiv:2311.11315},
  year={2023}
}
@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and others},
  journal={arXiv:2307.12966},
  year={2023}
}
@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and others},
  journal={arXiv:2308.10792},
  year={2023}
}
@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and others},
  journal={NeurIPS},
  year={2019}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv:2310.06825},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv:2302.13971},
  year={2023}
}
@inproceedings{mok2023large,
  title={Large-scale lifelong learning of in-context instructions and how to tackle it},
  author={Mok, Jisoo and Do, Jaeyoung and Lee, Sungjin and others},
  booktitle={ACL},
  year={2023}
}
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and others},
  journal={arXiv:2307.16789},
  year={2023}
}
@article{hao2023toolkengpt,
  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
  author={Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  journal={arXiv:2305.11554},
  year={2023}
}
@article{jin2023genegpt,
  title={Genegpt: Augmenting large language models with domain tools for improved access to biomedical information},
  author={Jin, Qiao and Yang, Yifan and Chen, Qingyu and Lu, Zhiyong},
  journal={arXiv:2304.09667},
  year={2023}
}
@article{liang2023taskmatrix,
  title={Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis},
  author={Liang, Yaobo and Wu, Chenfei and Song, Ting and  others},
  journal={arXiv:2303.16434},
  year={2023}
}
@article{dong2023abilities,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and others},
  journal={arXiv:2310.05492},
  year={2023}
}
@article{zhang2023reformulating,
  title={Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise},
  author={Zhang, Yating and Wang, Yexiang and Cheng, Fei and others},
  journal={arXiv:2310.03328},
  year={2023}
}
@misc{
cheng2023language,
title={Language model with Plug-in Knowledge Memory},
author={Xin Cheng and Yankai Lin and Dongyan Zhao and Rui Yan},
year={2023},
url={https://openreview.net/forum?id=Plr5l7r0jY6}
}
@article{cheng2023adapting,
  title={Adapting large language models via reading comprehension},
  author={Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
  journal={arXiv:2309.09530},
  year={2023}
}
@article{song2023conpet,
  title={Conpet: Continual parameter-efficient tuning for large language models},
  author={Song, Chenyang and Han, Xu and Zeng, Zheni and others},
  journal={arXiv:2309.14763},
  year={2023}
}
@article{zhao2024dapt,
  title={DAPT: A Dual Attention Framework for Parameter-Efficient Continual Learning of Large Language Models},
  author={Zhao, Weixiang and Wang, Shilong and Hu, Yulin and others},
  journal={arXiv:2401.08295},
  year={2024}
}
@article{wu2024llama,
  title={LLaMA Pro: Progressive LLaMA with Block Expansion},
  author={Wu, Chengyue and Gan, Yukang and Ge, Yixiao and others},
  journal={arXiv:2401.02415},
  year={2024}
}
@article{xiong2023rationale,
  title={Rationale-Enhanced Language Models are Better Continual Relation Learners},
  author={Xiong, Weimin and Song, Yifan and Wang, Peiyi and Li, Sujian},
  journal={arXiv:2310.06547},
  year={2023}
}
@article{jang2023exploring,
  title={Exploring the benefits of training expert language models over instruction tuning},
  author={Jang, Joel and Kim, Seungone and Ye, Seonghyeon and others},
  journal={arXiv:2302.03202},
  year={2023}
}
@article{ke2022continual,
  title={Continual training of language models for few-shot learning},
  author={Ke, Zixuan and Lin, Haowei and Shao, Yijia and Xu, Hu and Shu, Lei and Liu, Bing},
  journal={EMNLP},
  year={2022}
}
@article{razdaibiedina2023progressive,
  title={Progressive prompts: Continual learning for language models},
  author={Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and others},
  journal={arXiv:2301.12314},
  year={2023}
}
@inproceedings{
anonymous2024scalable,
title={Scalable Language Model with Generalized Continual Learning},
author={Anonymous},
booktitle={ICLR},
year={2024},
url={https://openreview.net/forum?id=mz8owj4DXu}
}
@article{shin2017continual,
  title={Continual learning with deep generative replay},
  author={Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  journal={NeurIPS},
  year={2017}
}
@article{yin2022contintin,
  title={Contintin: Continual learning from task instructions},
  author={Yin, Wenpeng and Li, Jia and Xiong, Caiming},
  journal={arXiv:2203.08512},
  year={2022}
}
@inproceedings{scialom2022fine,
  title={Fine-tuned language models are continual learners},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  booktitle={EMNLP},
  year={2022}
}
@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and others},
  journal={arXiv:2004.10964},
  year={2020}
}
@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "VLDB",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

@article{zhang2023citb,
  title={CITB: A Benchmark for Continual Instruction Tuning},
  author={Zhang, Zihan and Fang, Meng and Chen, Ling and Namazi-Rad, Mohammad-Reza},
  journal={arXiv:2310.14510},
  year={2023}
}


@article{gabriel_artificial_2020,
	title = {Artificial {Intelligence}, {Values}, and {Alignment}},
	journal = {Minds and Machines},
	author = {Gabriel, Iason},
	year = {2020},
}

@article{jain2023mechanistically,
  title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
  author={Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\"a}schel, Tim and Krueger, David Scott},
  journal={arXiv:2311.12786},
  year={2023}
}

@inproceedings{suhr2023continual,
title={Continual Learning for Instruction Following from Realtime Feedback},
author={Alane Suhr and Yoav Artzi},
booktitle={NeurIPS},
year={2023},
url={https://openreview.net/forum?id=ez6Cb0ZGzG}
}

@article{zhang2023copf,
  title={COPF: Continual Learning Human Preference through Optimal Policy Fitting},
  author={Zhang, Han and Gui, Lin and Zhai, Yuanzhao and others},
  journal={arXiv:2310.15694},
  year={2023}
}

@article{abs-2309-06256,
  title={Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models},
  author={Lin, Yong and Tan, Lu and Lin, Hangyu and others},
  journal={arXiv:2309.06256},
  year={2023}
}

@article{abs-2310-03693,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and others},
  journal={arXiv:2310.03693},
  year={2023}
}


@article{abs-2305-10120,
  title={Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models},
  author={Heng, Alvin and Soh, Harold},
  journal={arXiv:2305.10120},
  year={2023}
}

@inproceedings{ChenY23,
  author       = {Jiaao Chen and
                  Diyi Yang},
  title        = {Unlearn What You Want to Forget: Efficient Unlearning for LLMs},
  booktitle    = {EMNLP},
  year         = {2023},
}

@article{gupta2024model,
  title={Model Editing at Scale leads to Gradual and Catastrophic Forgetting},
  author={Gupta, Akshat and Rao, Anurag and Anumanchipalli, Gopala},
  journal={arXiv:2401.07453},
  year={2024}
}

@article{gu2024model,
  title={Model Editing Can Hurt General Abilities of Large Language Models},
  author={Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv:2401.04700},
  year={2024}
}

@article{abs-2307-09288,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{FedusZS22,
  author       = {William Fedus and
                  Barret Zoph and
                  Noam Shazeer},
  title        = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
                  and Efficient Sparsity},
  journal      = {J. Mach. Learn. Res.},
  year         = {2022},
  url          = {http://jmlr.org/papers/v23/21-0998.html},
}

@inproceedings{LewisPPPKGKLYR020,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and
                  others},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  booktitle    = {NeurIPS},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
}

@article{abs-2312-10997,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv:2312.10997},
  year={2023}
}

@article{liu2024tuning,
  title={Tuning Language Models by Proxy},
  author={Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A},
  journal={CoRR},
  year={2024}
}

@article{abs-2311-02428,
  author       = {Rajas Chitale and
                  Ankit Vaidya and
                  Aditya Kane and
                  Archana Ghotkar},
  title        = {Task Arithmetic with LoRA for Continual Learning},
  journal      = {CoRR},
  year         = {2023},
}

@inproceedings{KandpalDRWR23,
  author       = {Nikhil Kandpal and
                  Haikang Deng and
                  Adam Roberts and
                  Eric Wallace and
                  Colin Raffel},
  title        = {Large Language Models Struggle to Learn Long-Tail Knowledge},
  booktitle    = {ICML},
  year         = {2023},
}

@inproceedings{yao-etal-2023-editing,
    title = "Editing Large Language Models: Problems, Methods, and Opportunities",
    author = "Yao, Yunzhi  and
      Wang, Peng  and
      Tian, Bozhong  and
      others",
    booktitle = "EMNLP",
    year = "2023",
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={CoRR},
  year={2017}
}

@inproceedings{rafailov2023direct,
title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
booktitle={NeurIPS},
year={2023},
}

# ------- Survey ----------
@article{abs-2311-11908,
  author       = {Eli Verwimp and
                  Rahaf Aljundi and
                  Shai Ben{-}David and
                  Matthias Bethge and
                  others},
  title        = {Continual Learning: Applications and the Road Forward},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2302-00487,
  author       = {Liyuan Wang and
                  Xingxing Zhang and
                  Hang Su and
                  Jun Zhu},
  title        = {A Comprehensive Survey of Continual Learning: Theory, Method and Application},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2211-12701,
  author       = {Zixuan Ke and
                  Bing Liu},
  title        = {Continual Learning of Natural Language Processing Tasks: {A} Survey},
  journal      = {CoRR},
  year         = {2022},
}

@inproceedings{BiesialskaBC20,
  author       = {Magdalena Biesialska and
                  Katarzyna Biesialska and
                  Marta R. Costa{-}juss{\`{a}}},
  title        = {Continual Lifelong Learning in Natural Language Processing: {A} Survey},
  booktitle    = {COLING},
  year         = {2020},
}

@article{abs-2310-16218,
  author       = {Song Wang and
                  Yaochen Zhu and
                  Haochen Liu and
                  Zaiyi Zheng and
                  Chen Chen and
                  Jundong Li},
  title        = {Knowledge Editing for Large Language Models: {A} Survey},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2401-01286,
  author       = {Ningyu Zhang and
                  Yunzhi Yao and
                  Bozhong Tian and
                  others},
  title        = {A Comprehensive Study of Knowledge Editing for Large Language Models},
  journal      = {CoRR},
  year         = {2024},
}

@inproceedings{YaoWT0LDC023,
  author       = {Yunzhi Yao and
                  Peng Wang and
                  Bozhong Tian and
                  Siyuan Cheng and
                  Zhoubo Li and
                  Shumin Deng and
                  Huajun Chen and
                  Ningyu Zhang},
  title        = {Editing Large Language Models: Problems, Methods, and Opportunities},
  booktitle    = {EMNLP},
  year         = {2023},
}

@article{abs-2311-05876,
  author       = {Zhangyin Feng and
                  Weitao Ma and
                  Weijiang Yu and
                  others},
  title        = {Trends in Integration of Knowledge and Large Language Models: {A}
                  Survey and Taxonomy of Methods, Benchmarks, and Applications},
  journal      = {CoRR},
  year         = {2023},
}

@inproceedings{ZhangFCNW23,
  author       = {Zihan Zhang and
                  Meng Fang and
                  Ling Chen and
                  others},
  title        = {How Do Large Language Models Capture the Ever-changing World Knowledge?
                  {A} Review of Recent Advances},
  booktitle    = {EMNLP},
  year         = {2023},
}


@article{abs-2302-07842,
  author       = {Gr{\'{e}}goire Mialon and
                  Roberto Dess{\`{\i}} and
                  Maria Lomeli and
                  others},
  title        = {Augmented Language Models: a Survey},
  journal      = {CoRR},
  year         = {2023},
}

# ------- Pre-training ----------
# -------- Knowledge Upgrade ----
@inproceedings{JangYYSHKCS22,
  author       = {Joel Jang and
                  Seonghyeon Ye and
                  Sohee Yang and
                  others},
  title        = {Towards Continual Knowledge Learning of Language Models},
  booktitle    = {ICLR},
  year         = {2022},
}

@inproceedings{JinZZ00WA022,
  author       = {Xisen Jin and
                  Dejiao Zhang and
                  Henghui Zhu and
                  others},
  title        = {Lifelong Pretraining: Continually Adapting Language Models to Emerging
                  Corpora},
  booktitle    = {NAACL},
  year         = {2022},
}

@article{abs-2308-04014,
  author       = {Kshitij Gupta and
                  Benjamin Th{\'{e}}rien and
                  Adam Ibrahim and
                  others},
  title        = {Continual Pre-Training of Large Language Models: How to (re)warm your
                  model?},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2205-09357,
  author       = {Andrea Cossu and
                  Tinne Tuytelaars and
                  Antonio Carta and others},
  title        = {Continual Pre-Training Mitigates Forgetting in Language and Vision},
  journal      = {CoRR},
  year         = {2022},
}

@inproceedings{KeSLKK023,
  author       = {Zixuan Ke and
                  Yijia Shao and
                  Haowei Lin and
                  Tatsuya Konishi and
                  Gyuhak Kim and
                  Bing Liu},
  title        = {Continual Pre-training of Language Models},
  booktitle    = {ICLR},
  year         = {2023},
}


@article{abs-2401-03129,
  author       = {Chen{-}An Li and
                  Hung{-}Yi Lee},
  title        = {Examining Forgetting in Continual Pre-training of Aligned Large Language
                  Models},
  journal      = {CoRR},
  year         = {2024},
}

@inproceedings{ZhuWLZ023,
  author       = {Hongguang Zhu and
                  Yunchao Wei and
                  Xiaodan Liang and
                  Chunjie Zhang and
                  Yao Zhao},
  title        = {{CTP:} Towards Vision-Language Continual Pretraining via Compatible
                  Momentum Contrast and Topology Preservation},
  booktitle    = {ICCV},
  year         = {2023},
}

@inproceedings{YadavSDLZTBMNRB23,
  author       = {Prateek Yadav and
                  Qing Sun and
                  Hantian Ding and
                  others},
  title        = {Exploring Continual Learning for Code Generation Models},
  booktitle    = {ACL},
  year         = {2023},
}

@article{abs-2311-01200,
  author       = {Evangelia Gogoulou and
                  Timoth{\'{e}}e Lesort and
                  Magnus Boman and
                  Joakim Nivre},
  title        = {A Study of Continual Learning Under Language Shift},
  journal      = {CoRR},
  year         = {2023},
}


@article{abs-2312-15696,
  author       = {Shirong Ma and
                  Shen Huang and
                  Shulin Huang and
                  others},
  title        = {EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models
                  with Semi-structured Data},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2311-08545,
  author       = {Yong Xie and
                  Karan Aggarwal and
                  Aitzaz Ahmad},
  title        = {Efficient Continual Pre-training for Building Domain Specific Large
                  Language Models},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2310-10631,
  author       = {Zhangir Azerbayev and
                  Hailey Schoelkopf and
                  Keiran Paster and
                  others},
  title        = {Llemma: An Open Language Model For Mathematics},
  journal      = {CoRR},
  year         = {2023},
}

@article{abs-2401-01600,
  author       = {Xianjun Yang and
                  Junfeng Gao and
                  Wenxin Xue and
                  Erik Alexandersson},
  title        = {PLLaMa: An Open-source Large Language Model for Plant Science},
  journal      = {CoRR},
  year         = {2024},
}
@article{HuSSK23,
  author       = {Hexiang Hu and
                  Ozan Sener and
                  Fei Sha and
                  Vladlen Koltun},
  title        = {Drinking From a Firehose: Continual Learning With Web-Scale Natural
                  Language},
  journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  volume       = {45},
  number       = {5},
  year         = {2023},
}

@inproceedings{JangYLYSHKS22,
  author       = {Joel Jang and
                  Seonghyeon Ye and
                  Changho Lee and
                  others},
  title        = {TemporalWiki: {A} Lifelong Benchmark for Training and Evaluating Ever-Evolving
                  Language Models},
  booktitle    = {EMNLP},
  year         = {2022},
}

@inproceedings{YoonML023,
  author       = {Susik Yoon and
                  Yu Meng and
                  Dongha Lee and
                  Jiawei Han},
  title        = {SCStory: Self-supervised and Continual Online Story Discovery},
  booktitle    = {WWW},
  year         = {2023},
}

@inproceedings{OnoeZPDC23,
  author       = {Yasumasa Onoe and
                  Michael J. Q. Zhang and
                  Shankar Padmanabhan and
                  Greg Durrett and
                  Eunsol Choi},
  title        = {Can LMs Learn New Entities from Descriptions? Challenges in Propagating
                  Injected Knowledge},
  booktitle    = {ACL},
  year         = {2023},
}
@inproceedings{QinQHLWXLSZ23,
  author       = {Yujia Qin and
                  Cheng Qian and
                  Xu Han and
                  others},
  title        = {Recyclable Tuning for Continual Pre-training},
  booktitle    = {Findings of ACL},
  year         = {2023},
}
@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  year={1989},
}
@article{wang2023comprehensive,
  title={A comprehensive survey of continual learning: Theory, method and application},
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={CoRR},
  year={2023}
}
@inproceedings{CastellucciFC020,
  author       = {Giuseppe Castellucci and
                  Simone Filice and
                  Danilo Croce and
                  Roberto Basili},
  title        = {Learning to Solve {NLP} Tasks in an Incremental Number of Languages},
  booktitle    = {ACL},
  year         = {2021},
}

@article{MehtaPCS23,
  author       = {Sanket Vaibhav Mehta and
                  Darshan Patil and
                  Sarath Chandar and
                  Emma Strubell},
  title        = {An Empirical Investigation of the Role of Pre-training in Lifelong
                  Learning},
  journal      = {J. Mach. Learn. Res.},
  year         = {2023},
}

@inproceedings{SunWLFTWW20,
  author       = {Yu Sun and
                  Shuohuan Wang and
                  Yu{-}Kun Li and
                  others},
  title        = {{ERNIE} 2.0: {A} Continual Pre-Training Framework for Language Understanding},
  booktitle    = {AAAI},
  year         = {2020},
}

@inproceedings{ZanCYLKGWCL22,
  author       = {Daoguang Zan and
                  Bei Chen and
                  Dejian Yang and
                  others},
  title        = {{CERT:} Continual Pre-training on Sketches for Library-oriented Code
                  Generation},
  booktitle    = {IJCAI},
  year         = {2022},
}

@inproceedings{DaiLZ0HL23,
  author       = {Yi Dai and
                  Hao Lang and
                  Yinhe Zheng and
                  Bowen Yu and
                  Fei Huang and
                  Yongbin Li},
  title        = {Domain Incremental Lifelong Learning in an Open World},
  booktitle    = {Findings of ACL},
  year         = {2023},

}

@inproceedings{EthayarajhCS22,
  author       = {Kawin Ethayarajh and
                  Yejin Choi and
                  Swabha Swayamdipta},
  title        = {Understanding Dataset Difficulty with \emph{V}-Usable Information},
  booktitle    = {ICML},
  volume       = {162},
  year         = {2022},
}

@article{abs-2204-05862,
  author       = {Yuntao Bai and
                  Andy Jones and
                  Kamal Ndousse and
                  others},
  title        = {Training a Helpful and Harmless Assistant with Reinforcement Learning
                  from Human Feedback},
  journal      = {CoRR},
  year         = {2022},
}

@article{abs-2312-16337,
  author       = {Changmao Li and
                  Jeffrey Flanigan},
  title        = {Task Contamination: Language Models May Not Be Few-Shot Anymore},
  journal      = {CoRR},
  year         = {2023},
}

@inproceedings{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  booktitle={NeurIPS},
  year={2017}
}


@inproceedings{biesialska-etal-2020-continual,
    title = "Continual Lifelong Learning in Natural Language Processing: A Survey",
    author = "Biesialska, Magdalena  and
      Biesialska, Katarzyna  and
      Costa-juss{\`a}, Marta R.",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "COLING",
    year = "2020",
}


@article{abs-2311-12315,
  author       = {Shufa Wei and
                  Xiaolong Xu and
                  Xianbiao Qi and
                  others},
  title        = {AcademicGPT: Empowering Academic Research},
  journal      = {CoRR},
  year         = {2023},
}


# --------------------------------Tong
@inproceedings{WuLLHQZX21,
  author       = {Tongtong Wu and
                  Xuekai Li and
                  Yuan{-}Fang Li and
                  Gholamreza Haffari and
                  Guilin Qi and
                  Yujin Zhu and
                  Guoqiang Xu},
  title        = {Curriculum-Meta Learning for Order-Robust Continual Relation Extraction},
  booktitle    = {AAAI},
  year         = {2021},
}

@inproceedings{0002GWQLD23,
  author       = {Yongrui Chen and
                  Xinnan Guo and
                  Tongtong Wu and
                 others},
  title        = {Learn from Yesterday: {A} Semi-supervised Continual Learning Method
                  for Supervision-Limited Text-to-SQL Task Streams},
  booktitle    = {AAAI},
  year = {2023}
}

@article{abs-2305-08698,
  author       = {Xiang Chen and
                  Jintian Zhang and
                  Xiaohan Wang and
                  others},
  title        = {Continual Multimodal Knowledge Graph Construction},
  journal      = {CoRR},
  year         = {2023},
}

@article{he2024lifelong,
      title={Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning}, 
      author={Tao He and Tongtong Wu and Dongyang Zhang and Guiduo Duan and Ke Qin and Yuan-Fang Li},
      journal      = {CoRR},
      year={2024},
}

@inproceedings{WuCLLQH22,
  author       = {Tongtong Wu and
                  Massimo Caccia and
                  Zhuang Li and
                  Yuan{-}Fang Li and
                  Guilin Qi and
                  Gholamreza Haffari},
  title        = {Pretrained Language Model in Continual Learning: {A} Comparative Study},
  booktitle    = {ICLR},
  year         = {2022},
}
@article{qiao2024autoact,
      title={AUTOACT: Automatic Agent Learning from Scratch via Self-Planning}, 
      author={Shuofei Qiao and Ningyu Zhang and Runnan Fang and Yujie Luo and Wangchunshu Zhou and Yuchen Eleanor Jiang and Chengfei Lv and Huajun Chen},
      year={2024},
journal={CoRR}
}

@article{wu2024continual,
  title={Continual learning for large language models: A survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}
@article{li2024svdquant,
  title={Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models},
  author={Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  journal={arXiv preprint arXiv:2411.05007},
  year={2024}
}
@article{wang2023orthogonal,
  title={Orthogonal subspace learning for language model continual learning},
  author={Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2310.14152},
  year={2023}
}
@article{tian2024hydralora,
  title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning},
  author={Tian, Chunlin and Shi, Zhan and Guo, Zhijiang and Li, Li and Xu, Chengzhong},
  journal={arXiv preprint arXiv:2404.19245},
  year={2024}
}
@article{wu2024mixture,
  title={Mixture of lora experts},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2404.13628},
  year={2024}
}
@article{dou2023art,
  title={The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  year={2023}
}
@article{biderman2024lora,
  title={Lora learns less and forgets less},
  author={Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}
@article{jiang2024mora,
  title={MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning},
  author={Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and others},
  journal={arXiv preprint arXiv:2405.12130},
  year={2024}
}
@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{shuttleworth2024lora,
  title={LoRA vs Full Fine-tuning: An Illusion of Equivalence},
  author={Shuttleworth, Reece and Andreas, Jacob and Torralba, Antonio and Sharma, Pratyusha},
  journal={arXiv preprint arXiv:2410.21228},
  year={2024}
}
@article{meng2024pissa,
  title={Pissa: Principal singular values and singular vectors adaptation of large language models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}
@article{lingam2024svft,
  title={SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors},
  author={Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:2405.19597},
  year={2024}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}
@article{zi2023delta,
  title={Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices},
  author={Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
  journal={arXiv preprint arXiv:2309.02411},
  year={2023}
}
@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}
@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}
@article{balazy2024lora,
  title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}
@article{azizi2024lamda,
  title={LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation},
  author={Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud},
  journal={arXiv preprint arXiv:2406.12832},
  year={2024}
}
@article{buyukakyuz2024olora,
  title={OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models},
  author={B{\"u}y{\"u}kaky{\"u}z, Kerim},
  journal={arXiv preprint arXiv:2406.01775},
  year={2024}
}
@article{yang2024corda,
  title={CorDA: Context-Oriented Decomposition Adaptation of Large Language Models},
  author={Yang, Yibo and Li, Xiaojie and Zhou, Zhongzhu and Song, Shuaiwen Leon and Wu, Jianlong and Nie, Liqiang and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2406.05223},
  year={2024}
}
@article{wang2024milora,
  title={Milora: Harnessing minor singular components for parameter-efficient llm finetuning},
  author={Wang, Hanqing and Li, Yixia and Wang, Shuo and Chen, Guanhua and Chen, Yun},
  journal={arXiv preprint arXiv:2406.09044},
  year={2024}
}
@article{wang2024loraga,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}
@article{wang2024lorapro,
  title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?},
  author={Wang, Zhengbo and Liang, Jian and He, Ran and Wang, Zilei and Tan, Tieniu},
  journal={arXiv preprint arXiv:2407.18242},
  year={2024}
}
@article{paischer2024one,
  title={One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation},
  author={Paischer, Fabian and Hauzenberger, Lukas and Schmied, Thomas and Alkin, Benedikt and Deisenroth, Marc Peter and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2410.07170},
  year={2024}
}
@article{li2024svdqunat,
  title={SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},
  author={Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  journal={arXiv preprint arXiv:2411.05007},
  year={2024}
}
@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@article{gandhi2023distil,
  title={Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling},
  author={Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander M},
  journal={arXiv preprint arXiv:2311.00430},
  year={2023}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}
@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}
@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}
@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{shuttleworth2024lora,
  title={LoRA vs Full Fine-tuning: An Illusion of Equivalence},
  author={Shuttleworth, Reece and Andreas, Jacob and Torralba, Antonio and Sharma, Pratyusha},
  journal={arXiv preprint arXiv:2410.21228},
  year={2024}
}
@article{meng2024pissa,
  title={Pissa: Principal singular values and singular vectors adaptation of large language models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}
@article{lingam2024svft,
  title={SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors},
  author={Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:2405.19597},
  year={2024}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}
@article{zi2023delta,
  title={Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices},
  author={Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
  journal={arXiv preprint arXiv:2309.02411},
  year={2023}
}
@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}
@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}
@article{balazy2024lora,
  title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}
@article{azizi2024lamda,
  title={LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation},
  author={Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud},
  journal={arXiv preprint arXiv:2406.12832},
  year={2024}
}
@article{buyukakyuz2024olora,
  title={OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models},
  author={B{\"u}y{\"u}kaky{\"u}z, Kerim},
  journal={arXiv preprint arXiv:2406.01775},
  year={2024}
}
@article{yang2024corda,
  title={CorDA: Context-Oriented Decomposition Adaptation of Large Language Models},
  author={Yang, Yibo and Li, Xiaojie and Zhou, Zhongzhu and Song, Shuaiwen Leon and Wu, Jianlong and Nie, Liqiang and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2406.05223},
  year={2024}
}
@article{wang2024milora,
  title={Milora: Harnessing minor singular components for parameter-efficient llm finetuning},
  author={Wang, Hanqing and Li, Yixia and Wang, Shuo and Chen, Guanhua and Chen, Yun},
  journal={arXiv preprint arXiv:2406.09044},
  year={2024}
}
@article{wang2024loraga,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}
@article{wang2024lorapro,
  title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?},
  author={Wang, Zhengbo and Liang, Jian and He, Ran and Wang, Zilei and Tan, Tieniu},
  journal={arXiv preprint arXiv:2407.18242},
  year={2024}
}
@article{paischer2024one,
  title={One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation},
  author={Paischer, Fabian and Hauzenberger, Lukas and Schmied, Thomas and Alkin, Benedikt and Deisenroth, Marc Peter and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2410.07170},
  year={2024}
}
@article{li2024svdqunat,
  title={SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},
  author={Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  journal={arXiv preprint arXiv:2411.05007},
  year={2024}
}
@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@article{gandhi2023distil,
  title={Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling},
  author={Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander M},
  journal={arXiv preprint arXiv:2311.00430},
  year={2023}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
